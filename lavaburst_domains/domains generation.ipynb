{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lavaburst\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from os import makedirs\n",
    "import time\n",
    "import random\n",
    "from itertools import product\n",
    "from scipy.signal import argrelmax,argrelmin\n",
    "\n",
    "binsize = 25000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn order to get matrices, I used this script:\\n\\n\\nJUICER=/home/al/Juice/juicer.jar\\nStringVal=\"X 2R 2L 3R 3L\"\\nfor i in $StringVal\\ndo\\n    echo $i\\n    java -jar $JUICER dump observed -d KR http://genedev.bionet.nsc.ru/site/hic_out/Anopheles/hic/AcolNg_V3.1000.hic $i $i BP 25000 /home/al/Vp/Matrix25000/AcolNg1000_${i}_25000.txt\\ndone\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "In order to get matrices, I used this script:\n",
    "\n",
    "\n",
    "JUICER=/home/al/Juice/juicer.jar\n",
    "StringVal=\"X 2R 2L 3R 3L\"\n",
    "for i in $StringVal\n",
    "do\n",
    "    echo $i\n",
    "    java -jar $JUICER dump observed -d KR http://genedev.bionet.nsc.ru/site/hic_out/Anopheles/hic/AcolNg_V3.1000.hic $i $i BP 25000 /home/al/Vp/Matrix25000/AcolNg1000_${i}_25000.txt\n",
    "done\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/al/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "species = ['Aalb','Aatr','AcolNg','Amer','Aqmac','Aste','AmerA']\n",
    "chroms = ['X','2R','2L','3R','3L']\n",
    "# species = ['AcolNg']\n",
    "# chroms = ['X']\n",
    "\n",
    "ins_track = []\n",
    "specific_minima = []\n",
    "im_neighbor = []\n",
    "im_exact_intersect = []\n",
    "\n",
    "for sp, chrom in product(species,chroms):\n",
    "    A = np.loadtxt('/home/al/Vp/Matrix25000/'+sp+'1000_' + chrom + '_25000.txt')\n",
    "    A[np.isnan(A)] = 0\n",
    "    l = []\n",
    "    #Only four isolation window values are allowed in this code.\n",
    "    for i in [2,3,4,5]:\n",
    "        ins = lavaburst.utils.insul_diamond(A,i)\n",
    "        ins = pd.DataFrame(ins)\n",
    "        ins[1] = ins.index\n",
    "        ins[1] *= binsize\n",
    "        ins[2] = ins[1] + binsize -1\n",
    "        ins = ins[[1,2,0]]\n",
    "        ins.columns = [0,1,2]\n",
    "        ins.insert(0, 0, chrom, allow_duplicates=True)\n",
    "        ins.columns = [0,1,2,3]\n",
    "        fullins = np.array(ins)\n",
    "        ins_track.append([sp,i,fullins])\n",
    "        \n",
    "        ins[3] = ins[ins.columns[3]][(ins[ins.columns[3]].shift(1) > 0.02 + ins[ins.columns[3]]) & (ins[ins.columns[3]].shift(-1) > 0.02 + ins[ins.columns[3]])]\n",
    "        ins[3].loc[ins[3].notna()] = 1\n",
    "        ins.fillna(0,inplace=True)\n",
    "        ins = ins.reset_index(drop=True)\n",
    "        \n",
    "        part_mins = ins.loc[ins[3]==1]\n",
    "        part_mins = part_mins[[0,1,2]].reset_index(drop=True)\n",
    "#         part_mins = np.array(part_mins)\n",
    "        specific_minima.append([sp,i,part_mins])\n",
    "    \n",
    "        l.append(ins[3])\n",
    "    ins = pd.concat(l,axis=1)\n",
    "    ins.columns = [i for i in range(0,ins.shape[1])]\n",
    "    ins[len(ins.columns)] = ins.sum(axis=1)\n",
    "#     print(ins)\n",
    "    a = np.array(ins[4])\n",
    "    for i in range(1,len(a)):\n",
    "        if a[i]==3:\n",
    "#             print(a[i-1],a[i])\n",
    "            if a[i-1]==1:\n",
    "                a[i] += 1\n",
    "                a[i-1] -= 1\n",
    "        elif a[i]==2:\n",
    "            if a[i-1]==2:\n",
    "                a[i] += 2\n",
    "                a[i-1] -= 2 \n",
    "        elif a[i]==1:\n",
    "            if a[i-1]==3:\n",
    "                a[i] -= 1\n",
    "                a[i-1] += 1\n",
    "    ins1 = ins.loc[ins[4]>=4]\n",
    "    ins[4] = a\n",
    "    ins = ins.loc[ins[4]>=4]\n",
    "    \n",
    "    ins[0] = ins.index\n",
    "    ins1[0] = ins1.index\n",
    "    ins[0] = ins[0]*binsize\n",
    "    ins1[0] = ins1[0]*binsize\n",
    "    ins[1] = ins[0] + binsize -1\n",
    "    ins1[1] = ins1[0] + binsize -1\n",
    "    ins.insert(0, 0, chrom, allow_duplicates=True)\n",
    "    ins.columns = [i for i in range(0,ins.shape[1])]\n",
    "    ins1.insert(0, 0, chrom, allow_duplicates=True)\n",
    "    ins1.columns = [i for i in range(0,ins1.shape[1])]\n",
    "    ins = ins[[0,1,2]]\n",
    "    ins1 = ins1[[0,1,2]]\n",
    "    ins = np.array(ins)\n",
    "    ins1 = np.array(ins1)\n",
    "    im_neighbor.append([sp,ins])\n",
    "    im_exact_intersect.append([sp,ins1])\n",
    "\n",
    "ins_track = np.array(ins_track)\n",
    "specific_minima = np.array(specific_minima)\n",
    "im_neighbor = np.array(im_neighbor)\n",
    "im_exact_intersect = np.array(im_exact_intersect)\n",
    "\n",
    "ws = [2,3,4,5]\n",
    "\"\"\"\n",
    "Generates an insulation track for the specified window sizes.\n",
    "\"\"\"\n",
    "for sp,w in product(species,ws):\n",
    "    fi2 = ins_track[(ins_track[:,0]==str(sp))&(ins_track[:,1]==w)]\n",
    "    fi2 = fi2[:,2]\n",
    "    fi3 = np.concatenate(fi2,axis=0)\n",
    "    fi3 = pd.DataFrame(fi3)\n",
    "    fi3 = fi3.sort_values([0,1,2]).reset_index(drop=True)\n",
    "#     np.savetxt('/home/al/Vp/Ins/'+sp+'_ins'+str(w)+'.bedGraph', fi3, delimiter='\\t', newline='\\n',fmt='%s')\n",
    "\n",
    "\"\"\"\n",
    "\"Extended\" intersection of insulation minima between four window sizes.\n",
    "\n",
    "For example, if for window sizes w = [2,4,5], \n",
    "insulation minimums are in the bin with the coordinate \"x\", \n",
    "but for window size w = [3], minimums are in the bin with the coordinate \"x-1\", \n",
    "such a missmatch is not discarded.\n",
    "\n",
    "The bin \"x\" is considered to have a minimum that intersects for all four windows.\n",
    "The scheme of the algorithm:\n",
    "(1|3 = 0|4)\n",
    "(2|2 = 0|4)\n",
    "(3|1 = 4|0)\n",
    "\n",
    "\"\"\"\n",
    "for sp in species:\n",
    "    immw = im_neighbor[im_neighbor[:,0]==str(sp)]\n",
    "    immw = immw[:,1]\n",
    "    immw2 = np.concatenate(immw,axis=0)\n",
    "#     np.savetxt('/home/al/Vp/IM/'+sp+'_IM.bed', immw2, delimiter='\\t', newline='\\n',fmt='%s')\n",
    "\n",
    "#Exact intersection of insulation minima between several window sizes.\n",
    "# for sp in species:\n",
    "#     imm = im_exact_intersect[im_exact_intersect[:,0]==str(sp)]\n",
    "#     imm = imm[:,1]\n",
    "#     imm2 = np.concatenate(imm,axis=0)\n",
    "# #     np.savetxt('/home/al/Vp/IMT/'+sp+'_IMwout.bed', imm2, delimiter='\\t', newline='\\n',fmt='%s')\n",
    "  \n",
    "# #Create insulation minima for a specific window size.\n",
    "# for sp,w in product(species,ws):\n",
    "#     part_m1 = specific_minima[(specific_minima[:,0]==str(sp))&(specific_minima[:,1]==w)]\n",
    "#     part_m1 = part_m1[:,2]\n",
    "#     part_m2 = np.concatenate(part_m1,axis=0)\n",
    "#     part_m2 = pd.DataFrame(part_m2)\n",
    "#     part_m2 = part_m2.sort_values([0,1,2]).reset_index(drop=True)\n",
    "\n",
    "# #     np.savetxt('/home/al/Vp/IMT29/'+sp+'_IM'+str(w)+'.bed', part_m2, delimiter='\\t', newline='\\n',fmt='%s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = ['Aalb','Aatr','AcolNg','Amer','Aqmac','Aste','AmerA']\n",
    "chroms = ['X','2R','2L','3R','3L']\n",
    "species = ['Aste']\n",
    "chroms = ['X']\n",
    "sp_delta = []\n",
    "\"\"\"\n",
    "Generates a delta vector according to:\n",
    "https://www.ncbi.nlm.nih.gov/pubmed/26030525\n",
    "\n",
    "Insulation diamond window: 5 bins.\n",
    "Delta window: 5 bins.\n",
    "\n",
    "Delta vector = (mean (2 left bins)) - (mean (2 right bins)).\n",
    "\n",
    "\"\"\"\n",
    "for sp, chrom in product(species,chroms):\n",
    "    ins = pd.read_csv('/home/al/Vp/Ins/'+sp+'_ins5.bedGraph',sep='\\t',header = None)\n",
    "    f_vals = ins.loc[(ins[0] == chrom)].reset_index(drop=True)\n",
    "    f_vals1 = pd.DataFrame(f_vals[3])\n",
    "    f_vals1[1] = f_vals.index\n",
    "    f_vals1 = f_vals1[[1,3]]\n",
    "    f_vals1.columns = [0,1]\n",
    "    \n",
    "    insmins = f_vals1.copy()\n",
    "    insmins[1] = f_vals1[f_vals1.columns[1]][(f_vals1[f_vals1.columns[1]].shift(1) > f_vals1[f_vals1.columns[1]]) \n",
    "                                          & (f_vals1[f_vals1.columns[1]].shift(-1) > f_vals1[f_vals1.columns[1]])]\n",
    "    insmins = insmins.dropna()\n",
    "    insmins = insmins.reset_index(drop=True)\n",
    "    insmins = insmins[0].values\n",
    "\n",
    "    f_vals1[2] = f_vals1[1].shift(1)\n",
    "    f_vals1[3] = f_vals1[1].shift(2)\n",
    "    f_vals1[4] = f_vals1[1].shift(3)\n",
    "    f_vals1[5] = f_vals1[1].shift(-1)\n",
    "    f_vals1[6] = f_vals1[1].shift(-2)\n",
    "    f_vals1[7] = f_vals1[1].shift(-3)\n",
    "    f_vals1[8] = f_vals1[[2,3]].mean(axis=1,skipna = False)\n",
    "    f_vals1[9] = f_vals1[[5,6]].mean(axis=1,skipna = False)\n",
    "\n",
    "    f_vals1 = f_vals1[[0,1,8,9]]\n",
    "    f_vals1.columns = [0,1,2,3]\n",
    "    f_vals1[4] = f_vals1[2] - f_vals1[3]\n",
    "    f_vals1.drop(f_vals1.head(3).index,inplace=True)\n",
    "    f_vals1.drop(f_vals1.tail(3).index,inplace=True)\n",
    "    f_vals1[5] = f_vals1[4].shift(-1)\n",
    "    f_vals1[6] = 0\n",
    "    f_vals1[6].loc[f_vals1.loc[f_vals1[4]>=0].loc[f_vals1[5]<=0].index] += 1\n",
    "    f_vals2 = f_vals1.loc[f_vals1[6]==1]\n",
    "    mins = f_vals2[0]\n",
    "    startend = pd.DataFrame(data = {0:[0,max(f_vals1[0])]})\n",
    "    startend.index = startend[0].tolist()\n",
    "    mins = mins.append(startend).sort_values([0])\n",
    "    mins[1] = mins[0].shift(-1)\n",
    "    mins[2] = mins[0].shift(-2)\n",
    "    mins.drop(mins.tail(2).index,inplace=True)\n",
    "    \n",
    "    armin = np.array(mins)\n",
    "    arref = np.array(f_vals1)\n",
    "    ar2 = []\n",
    "    for m in armin:\n",
    "        m0 = m[0]\n",
    "        m1 = m[1]\n",
    "        m2 = m[2]\n",
    "        leftmax = max(arref[(arref[:,0]>=m0) & (arref[:,0]<=m1)][:,4])\n",
    "        if (leftmax < 0.05):\n",
    "            continue\n",
    "        rightmin = min(arref[(arref[:,0]>=m1) & (arref[:,0]<=m2)][:,4])\n",
    "        if (rightmin > -0.05):\n",
    "            continue\n",
    "        #boundary_strenght\n",
    "        bs = leftmax-rightmin\n",
    "        bs_sn = 1. / (1 + np.exp(-bs)) * 2 - 1\n",
    "        if (bs_sn < 0.1):\n",
    "            continue\n",
    "        if m1+1 in insmins:\n",
    "            m1 += 1\n",
    "        ar2.append([m1,bs,bs_sn])\n",
    "    ar2 = pd.DataFrame(ar2)\n",
    "\n",
    "    ar2.loc[:,0] *= binsize\n",
    "    ar2[3] = ar2[0]+binsize-1\n",
    "    ar3 = ar2[[0,3,2]]\n",
    "    ar3.insert(0, 0, chrom, allow_duplicates=True)\n",
    "    ar3.columns = [0,1,2,3]\n",
    "    ar3 = ar3.astype({0:str,1:int,2:int})\n",
    "    sp_delta.append([sp,ar3])\n",
    "sp_delta = np.array(sp_delta)\n",
    "\n",
    "for sp in species:\n",
    "    sp_delta1 = sp_delta[sp_delta[:,0]==str(sp)]\n",
    "    sp_delta2 = sp_delta1[:,1]\n",
    "    sp_delta3 = np.concatenate(sp_delta2,axis=0)\n",
    "#     np.savetxt('/home/al/Vp/Delta/'+sp+'_delta.bed', sp_delta3, delimiter='\\t', newline='\\n',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35\n",
      "Aste 3L 105 4.486111111111111 2.5371736111111107\n",
      "end of chrom Aste 3L\n",
      "11.1757 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Requirements:\n",
    "- lavaburst package https://github.com/nvictus/lavaburst ;\n",
    "- Dense HiC matrix;\n",
    "- File with restricted starts/ends for each chromosome;\n",
    "- Insulation vector;\n",
    "- Insulation minima (intersected* between four window sizes);\n",
    "- Delta vector.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def consensus_domains(segments, weights):\n",
    "    \"\"\"\n",
    "    Returns consensus list of nonoverlapping segments.\n",
    "    Segments are 2-tuples given as half-open intervals [a,b).\n",
    "\n",
    "    \"\"\"\n",
    "    occ = defaultdict(int)\n",
    "    for d, w in zip(segments, weights):\n",
    "        occ[d] += w\n",
    "    # map each domain to its closest non-overlapping predecessor\n",
    "    M = len(segments)\n",
    "    seg = pd.DataFrame(segments)\n",
    "    prev = np.zeros(M, dtype=int)\n",
    "    for i in range(M-1, -1, -1):\n",
    "        d = segments[i]\n",
    "        j = i - 1\n",
    "        while j > -1:\n",
    "            if segments[j][1] <= d[0]: \n",
    "                prev[i] = j\n",
    "                break\n",
    "            j -= 1\n",
    "    pr = pd.DataFrame(prev)\n",
    "    # weighted interval scheduling dynamic program\n",
    "    score = np.zeros(M, dtype=float)\n",
    "    for i in range(1, M):\n",
    "        d = segments[i]\n",
    "        s_choose = score[prev[i]] + occ[d]\n",
    "        s_ignore = score[i-1]\n",
    "        score[i] = max(s_choose, s_ignore)\n",
    "    consensus = []\n",
    "    j = M - 1\n",
    "    while j > 0:\n",
    "        if score[j] != score[j-1]:\n",
    "            consensus.append(segments[j])\n",
    "            j = prev[j]\n",
    "        else:\n",
    "            j -= 1\n",
    "    return consensus[::-1], max(score)\n",
    "\n",
    "\n",
    "def sets_generator (A, g1, g2, gstep):\n",
    "    \"\"\"\n",
    "    g1 - min gamma, g2 - max gamma.\n",
    "    \"\"\"\n",
    "    seglist = []\n",
    "    for g in np.arange(g1,g2,gstep): \n",
    "        S1 = lavaburst.scoring.modularity_score(A, gamma=g)\n",
    "        model1 = lavaburst.SegModel(S1)\n",
    "        segs,sc = model1.optimal_segmentation(return_score=True)\n",
    "        sc = np.array(sc,ndmin=2).transpose()\n",
    "        segs = np.concatenate([segs,sc],axis=1)\n",
    "        seglist.append(segs)\n",
    "    return seglist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def column_insul(col):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function checks if the boundaries of the TAD intersect with\n",
    "    minima of insulation or delta minima for each TAD border.\n",
    "\n",
    "    It returns the insulation values at the border and \n",
    "    the intersection scores with the corresponding minima.\n",
    "    \"\"\"\n",
    "    \n",
    "    insul_minima_score = 0.25\n",
    "    delta_minima_score = 0.5\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['ins_values'] = f_vals1.loc[col][1].reset_index(drop=True)\n",
    "    df[2] = 0\n",
    "    df.fillna(0, inplace = True)\n",
    "    df = df.set_index(col.values,drop=True)\n",
    "\n",
    "\n",
    "    df['ins_minima'] = 0\n",
    "    bm = np.intersect1d(col,insmins[:,0])\n",
    "    df.loc[bm,'ins_minima'] = insul_minima_score\n",
    "    \n",
    "\n",
    "\n",
    "    df['delta_minima'] = 0\n",
    "    bd = np.intersect1d(col,deltas[:,0])\n",
    "    df.loc[bd,'delta_minima'] = delta_minima_score\n",
    "    \n",
    "#     #Don't use.\n",
    "#     am = np.intersect1d(col-1,insmins[:,0])\n",
    "#     cm = np.intersect1d(col+1,insmins[:,0])\n",
    "#     df.loc[(am+1),3] = 0.25\n",
    "#     df.loc[(cm-1),3] = 0.25\n",
    "#     ad = np.intersect1d(col-1,deltas[:,0])\n",
    "#     cd = np.intersect1d(col+1,deltas[:,0])\n",
    "#     df.loc[(ad+1),4] = 2\n",
    "#     df.loc[(cd-1),4] = 2\n",
    "    return df['ins_values'].values, df['delta_minima'].values, df['ins_minima'].values\n",
    "\n",
    "\n",
    "def modul_score(SA,table):\n",
    "    \"\"\"\n",
    "    The function assigns scores to the domain \n",
    "    if the domain belongs to the corresponding percentile interval. \n",
    "\n",
    "    The function takes an array of segment aggregation (modularity) score,\n",
    "    a list of lists [[percentile start value, percentile end value, score]]\n",
    "    and returns corresponding modularity TAD score. \n",
    "\n",
    "    i[0],i[1] - left, right percentile boundary values of modularity;\n",
    "    i[2] - percentile interval score.\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i in table:\n",
    "        sc = np.copy(SA)\n",
    "        sc[(sc>=i[0])&(sc<i[1])] = i[2]\n",
    "        sc[(sc!=i[2])]=0\n",
    "        l.append(sc)\n",
    "    sc1 = sum(l)\n",
    "    return sc1\n",
    "\n",
    "species = ['Aalb', 'Aatr', 'AcolNg', 'Amer', 'Aqmac', 'AmerA']\n",
    "chroms = ['X', '2R', '2L', '3R', '3L']\n",
    "species = ['Aste']\n",
    "chroms = ['3L']\n",
    "\n",
    "\"\"\"\n",
    "Set the range of gamma parameter.\n",
    "\"\"\"\n",
    "gg1 = [4]\n",
    "gg2 = [x for x in range(15, 35, 3)]\n",
    "gg2 = [x for x in range(35, 90, 5)]\n",
    "gg2 = gg2 + [x for x in range(80, 180, 20)]\n",
    "gg2 = gg2 + [x for x in range(180, 240, 30)]\n",
    "gg2 = gg2 + [x for x in range(240, 600, 100)]\n",
    "gg2 = [105] \n",
    "ggstep = [1]\n",
    "\n",
    "\"\"\"\n",
    "Set the number of domain sets with different average tad lengths.\n",
    "\"\"\"\n",
    "sizes = [7.2, 5.6, 4.35]\n",
    "\n",
    "meta = []\n",
    "start_time = time.process_time()\n",
    "for sp, chrom in product(species,chroms):\n",
    "    #Restricted start/end for each chromosome.\n",
    "    rchr = pd.read_csv('/home/al/Vp/RSizes/'+sp+'.sizes',sep='\\t',header = None)\n",
    "    rchr1 = rchr.loc[(rchr[0] == chrom)]\n",
    "    rchr1 = rchr1.reset_index(drop=True)\n",
    "    r_start = int(rchr1[1][0]/binsize)\n",
    "    r_end = int(rchr1[2][0]/binsize)\n",
    "    assert r_start > 0\n",
    "\n",
    "    ins = pd.read_csv('/home/al/Vp/Ins/'+sp+'_ins4.bedGraph',sep='\\t',header = None)\n",
    "    mins = pd.read_csv('/home/al/Vp/IM/'+sp+'_IM.bed',sep='\\t',header = None) \n",
    "    deltas = pd.read_csv('/home/al/Vp/Delta/'+sp+'_delta.bed',sep='\\t',header = None) \n",
    "\n",
    "    B = pd.read_csv('/home/al/Vp/Matrix25000/'+sp+'1000_'+chrom+'_25000.txt',delimiter='\\t',header = None)\n",
    "    B.drop(B.columns[len(B.columns)-1], axis=1, inplace=True)\n",
    "    assert B.shape[0] == B.shape[1]\n",
    "    A = np.array(B)    \n",
    "    A[np.isnan(A)] = 0\n",
    "\n",
    "    f_vals = ins.loc[(ins[0] == chrom)].reset_index(drop=True)\n",
    "    f_vals1 = pd.DataFrame(f_vals[3])\n",
    "    f_vals1[1] = f_vals.index\n",
    "    f_vals1 = f_vals1[[1,3]]\n",
    "    f_vals1.columns = [0,1]\n",
    "    ff = np.array(f_vals1)\n",
    "\n",
    "    nmins = f_vals1.copy()\n",
    "    nmins[1] = f_vals1[f_vals1.columns[1]][(f_vals1[f_vals1.columns[1]].shift(1) > f_vals1[f_vals1.columns[1]]) \n",
    "                                          & (f_vals1[f_vals1.columns[1]].shift(-1) > f_vals1[f_vals1.columns[1]])]\n",
    "    nmins = nmins.dropna()\n",
    "    nmins = nmins.reset_index(drop=True)\n",
    "\n",
    "    insmins = mins.loc[(mins[0] == chrom)]\n",
    "    insmins = pd.DataFrame(insmins[1])\n",
    "    insmins.columns = [0]\n",
    "    insmins /= binsize\n",
    "    insmins = f_vals1.loc[insmins[0].values].reset_index(drop=True)\n",
    "    insmins = np.array(insmins)\n",
    "\n",
    "    deltas = deltas.loc[(deltas[0] == chrom)]\n",
    "    deltas = pd.DataFrame(deltas[[1,3]]).reset_index(drop=True)\n",
    "    deltas.columns = [0,1]\n",
    "    deltas[0] /= binsize\n",
    "    deltas = np.array(deltas)\n",
    "    \n",
    "    \"\"\"\n",
    "    Alternative domains between two insulation minima.\n",
    "    Domain quality is assessed \n",
    "    by the value of a strong maximum of insulation inside \n",
    "    minus the average value of insulation at the borders\n",
    "    and by the absence of delta minima inside.\n",
    "    Experimental feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    dmins = nmins.copy()\n",
    "    dmins[1] = dmins[0].shift(-1)\n",
    "    dmins = dmins.dropna()\n",
    "    dmins[2] = 0\n",
    "    dmins = dmins.loc[dmins[0]>=r_start].reset_index(drop=True)\n",
    "    dmins = dmins.loc[dmins[1]<r_end]\n",
    "    dmins = dmins.astype({0:int,1:int})\n",
    "    dmins1 = dmins[[0,1]].apply(column_insul,axis=0)\n",
    "    dmins1 = np.transpose(np.concatenate(dmins1))\n",
    "    dmins1 = pd.DataFrame(dmins1)\n",
    "    dmins1[6] = dmins1[[2,5]].sum(axis=1)\n",
    "    dmins1[7] = dmins1[[1,4]].sum(axis=1)\n",
    "    dmins1 = pd.concat([dmins[[0,1,2]],dmins1],axis=1)\n",
    "    dmins1.columns = [x for x in range(0,dmins1.shape[1])]\n",
    "    dmins1 = dmins1[[0,1,2,3,6,9,10]]\n",
    "    dmins1.columns = [x for x in range(0,dmins1.shape[1])]\n",
    "\n",
    "    aaar11 = np.array(dmins1)\n",
    "    aaar22 = []\n",
    "    for i in aaar11:\n",
    "        x = i[0]\n",
    "        y = i[1]\n",
    "        ix = i[3]\n",
    "        iy = i[4]\n",
    "        ibmean = (ix+iy)/2\n",
    "\n",
    "        try:\n",
    "            ff1 = ff[(ff[:,0]>=x) & (ff[:,0]<=y)]\n",
    "            maxins1 = max(ff1[argrelmax(ff1[:,1])][:,1])\n",
    "            maxins1 = maxins1 - ibmean\n",
    "        except IndexError:\n",
    "            maxins1 = 0\n",
    "        except ValueError:\n",
    "            maxins1 = 0\n",
    "            \n",
    "        try:\n",
    "            dlt_inside = max(deltas[(deltas[:,0]>x+1) & (deltas[:,0]<y-1)][:,1])\n",
    "        except IndexError:\n",
    "            dlt_inside = 0\n",
    "        except ValueError:\n",
    "            dlt_inside = 0\n",
    "\n",
    "        if dlt_inside > 0.15:\n",
    "            d_insd = -9\n",
    "        elif (dlt_inside >0) & (dlt_inside<=0.15):\n",
    "            d_insd = -4\n",
    "        else:\n",
    "            d_insd = 0\n",
    "        i = np.append(i,[0,d_insd,maxins1]).tolist()\n",
    "        aaar22.append(i)\n",
    "\n",
    "    dmins = np.array(aaar22)\n",
    "    small = dmins[(dmins[:,1]-dmins[:,0])<4]\n",
    "    small75p = np.percentile(small[:,9],75)\n",
    "    small90p = np.percentile(small[:,9],90)\n",
    "    small75 = small[(small[:,9]>small75p)&(small[:,9]<=small90p)]\n",
    "    small90 = small[small[:,9]>small90p]\n",
    "    small75[:,9] = 0.5\n",
    "    small90[:,9] = 2\n",
    "\n",
    "    big = dmins[(dmins[:,1]-dmins[:,0])>=4]\n",
    "    big90p = np.percentile(big[:,9],90)\n",
    "    big80p = np.percentile(big[:,9],80)\n",
    "    big70p = np.percentile(big[:,9],70)\n",
    "    big60p = np.percentile(big[:,9],60)\n",
    "    big50p = np.percentile(big[:,9],50)\n",
    "    big90 = big[big[:,9]>big90p]\n",
    "    big80 = big[(big[:,9]>big80p)&(big[:,9]<=big90p)]\n",
    "    big70 = big[(big[:,9]>big70p)&(big[:,9]<=big80p)]\n",
    "    big60 = big[(big[:,9]>big60p)&(big[:,9]<=big70p)]\n",
    "    big50 = big[(big[:,9]>big50p)&(big[:,9]<=big60p)]\n",
    "    big50[:,9] = 0.5\n",
    "    big60[:,9] = 0.75\n",
    "    big70[:,9] = 1.5\n",
    "    big80[:,9] = 2.5\n",
    "    big90[:,9] = 3\n",
    "\n",
    "    dmins = np.concatenate([small75,small90,big50,big60,big70,big80,big90],axis=0)\n",
    "\n",
    "    dmins = pd.DataFrame(dmins)\n",
    "    \n",
    "    dmins[10] = 0\n",
    "    dmins[11] = dmins[[5,6,7,8,9,10]].sum(axis=1)\n",
    "    dmins[11].iloc[dmins.loc[dmins[11] == 0].index]=0.001\n",
    "    dmins = dmins[[0,1,2,3,4,5,6,7,8,10,9,11]]\n",
    "    dmins.columns = range(dmins.shape[1])\n",
    "    \n",
    "    for g1,g2,gstep in product(gg1,gg2,ggstep):\n",
    "        seglist= sets_generator(A,g1,g2+0.01,gstep)\n",
    "\n",
    "        updsegs = []\n",
    "        for seg in seglist:\n",
    "            seg = seg[seg[:,0]>=r_start]\n",
    "            seg = seg[seg[:,1]<r_end]\n",
    "            seg = seg[(seg[:,1]-seg[:,0])>=2]\n",
    "            try:\n",
    "                start = seg[:,0].min()\n",
    "            except ValueError:\n",
    "                continue\n",
    "            end = seg[:,1].max()\n",
    "            seg = pd.DataFrame(seg)\n",
    "            seg = seg.astype({0:int,1:int})\n",
    "\n",
    "            df1 = seg[[0,1]].apply(column_insul,axis=0)\n",
    "            df1 = np.transpose(np.concatenate(df1))\n",
    "            df1 = pd.DataFrame(df1)\n",
    "            \n",
    "            df1[6] = df1[[2,5]].sum(axis=1)\n",
    "            df1[7] = df1[[1,4]].sum(axis=1)\n",
    "            \n",
    "            df1 = pd.concat([seg[[0,1,2]],df1],axis=1)\n",
    "            df1.columns = [x for x in range(0,df1.shape[1])]\n",
    "            df1 = df1[[0,1,2,3,6,9,10]]\n",
    "            df1.columns = [x for x in range(0,df1.shape[1])]\n",
    "            ar1 = np.array(df1)\n",
    "            ar2 = []\n",
    "            for i in ar1:\n",
    "                x = i[0]\n",
    "                y = i[1]\n",
    "                l = y-x\n",
    "                ix = i[3]\n",
    "                iy = i[4]\n",
    "                ibmean = (ix+iy)/2\n",
    "                \n",
    "                \n",
    "                if l >= 5:\n",
    "                    #For each insulation minima inside the large TAD ( in (x+1,y-1) ):\n",
    "                    # -0.25 to the total scores of this TAD.\n",
    "                    try:\n",
    "                        minnumb = insmins[(insmins[:,0]>(x+1)) & (insmins[:,0]<(y-1))][:,1]\n",
    "                        minnumb = minnumb[minnumb[:] < ibmean]\n",
    "                        minnumb = -len(minnumb)*0.25\n",
    "                    except IndexError:\n",
    "                        minnumb = 0\n",
    "                    except ValueError:\n",
    "                        minnumb = 0\n",
    "                else:\n",
    "                    minnumb = 0\n",
    "                #IMM = minimal insulation minimum inside the TAD.\n",
    "                try:\n",
    "                    imm = min(ff[(ff[:,0]>(x+1)) & (ff[:,0]<(y-1))][:,1])\n",
    "                except IndexError:\n",
    "                    imm = 26\n",
    "                except ValueError:\n",
    "                    imm = 26\n",
    "                \n",
    "                #If IMM is lower than the insulation value at each of the boundaries, \n",
    "                #then -3 to the total scores. \n",
    "                #...at one of the boundaries, then -1 to the total scores.\n",
    "                if imm == 26:\n",
    "                    inside = 0\n",
    "                else:\n",
    "                    if l >= 5:\n",
    "                        if ((imm - ix)<0) & ((imm - iy)<0):\n",
    "                            inside = -3\n",
    "                        elif ((imm - ix)<0) & ((imm - iy)>=0):\n",
    "                            inside = -1\n",
    "                        elif ((imm - ix)>=0) & ((imm - iy)<0):\n",
    "                            inside = -1\n",
    "                        else:\n",
    "                            inside = 0\n",
    "                    else:\n",
    "                        inside = 0\n",
    "\n",
    "                try:\n",
    "                    dlt_inside = max(deltas[(deltas[:,0]>(x+1)) & (deltas[:,0]<(y-1))][:,1])\n",
    "                except IndexError:\n",
    "                    dlt_inside = 0\n",
    "                except ValueError:\n",
    "                    dlt_inside = 0\n",
    "                \n",
    "                #For each strong delta minima (boundary strength >0.2)\n",
    "                #inside the large TAD ( in (x+1,y-1) ):\n",
    "                # -0.25 to the total scores of this TAD.\n",
    "                if dlt_inside > 0.2:\n",
    "                    d_insd = -9\n",
    "                elif (dlt_inside >0) & (dlt_inside<=0.2):\n",
    "                    d_insd = 0\n",
    "                else:\n",
    "                    d_insd = 0\n",
    "                i = np.append(i,[inside,d_insd,minnumb]).tolist()\n",
    "                ar2.append(i)\n",
    "                \n",
    "            arr = np.array(ar2)\n",
    "            arr = np.hstack((arr,np.zeros((len(arr),1))))\n",
    "            #See modul_score function description above.\n",
    "            moduls = arr[:,2]\n",
    "            p10 = np.percentile(moduls, 10)\n",
    "            p25 = np.percentile(moduls, 25)\n",
    "            p50 = np.percentile(moduls, 50)\n",
    "            p75 = np.percentile(moduls, 75)\n",
    "            p85 = np.percentile(moduls, 85)\n",
    "            p90 = np.percentile(moduls, 90)\n",
    "            p95 = np.percentile(moduls, 95)\n",
    "            \n",
    "            table = [\n",
    "                [0, p10, -5],\n",
    "                [p10, p25, -1],\n",
    "                [p25, p50, 0],\n",
    "                [p50, p75, 1],\n",
    "                [p75, p85, 2],\n",
    "                [p85, p90, 2.5],\n",
    "                [p90, p95, 3],\n",
    "                [p95, 1, 5],\n",
    "            ]\n",
    "            #SA = segment_aggregation parameter (modularity)\n",
    "            arr[:,10] = modul_score(arr[:,2],table)\n",
    "            dfg = pd.DataFrame(arr)\n",
    "            updsegs.append(dfg)\n",
    "        df3 = pd.concat(updsegs,axis=0,ignore_index = True)\n",
    "        \n",
    "        #Balancing large TADs.\n",
    "        df3[10].loc[(df3[1]-df3[0])>8] *= 1.5\n",
    "        df3[10].loc[(df3[1]-df3[0])>14] *= 1.25\n",
    "        \n",
    "        df3[11] = df3[[5,6,7,8,9,10]].sum(axis=1)\n",
    "        #Merge with domains called from insulation minima.\n",
    "        \n",
    "        df3 = pd.concat([df3,dmins],axis=0,ignore_index = True)\n",
    "        # Values of maximal insulation inside the domain\n",
    "        # included in the column \"SA_score\"\n",
    "        # for the domains called from insulation minima.\n",
    "        \n",
    "        df3[11].iloc[df3.loc[df3[11] == 0].index]=0.001\n",
    "        df3[11].loc[(df3[1]-df3[0])<4] /= 2\n",
    "        \n",
    "        df3.columns = ['X','Y','Modul','Insul_x','Insul_y','IM_border','DLT_border',\n",
    "                       'IMM_inside','DLT_inside','IM_inside','SA_score','Total']\n",
    "#         print(df3)\n",
    "        df3 = df3.sort_values('Total',ascending=False).reset_index(drop=True)\n",
    "        df3 = df3.loc[df3.duplicated(subset=['X','Y'],keep='first') != True]\n",
    "\n",
    "        df6 = df3.sort_values(['X','Y'], ascending=True).reset_index(drop=True)\n",
    "        df6[['X','Y']] *= binsize\n",
    "        df6 = np.array(df6)\n",
    "        df6 = np.round(df6,5)\n",
    "#         for i in df6:\n",
    "#             i = i.tolist()\n",
    "#             print(i)\n",
    "        df3 = df3.sort_values(['Y'], ascending=True).reset_index(drop=True)\n",
    "        df3 = df3[['X','Y','Total']]\n",
    "        df3.columns = ['start','end','score']\n",
    "        xylist = df3[['start','end']].values.tolist()\n",
    "        xylist = [tuple(k) for k in xylist]\n",
    "        scorelist =  df3['score'].values.tolist()\n",
    "        listofcons,max_sc = consensus_domains(xylist,scorelist)\n",
    "        \n",
    "        consensus_df = pd.DataFrame(listofcons).sort_values([0,1], ascending=True).reset_index(drop=True)\n",
    "        tad_len = np.mean(consensus_df[1] - consensus_df[0])\n",
    "        \n",
    "        consensus_df[0] = consensus_df[0]*binsize\n",
    "        consensus_df[1] = consensus_df[1]*binsize-1\n",
    "        consensus_df.insert(0, 0, chrom, allow_duplicates=True)\n",
    "        consensus_df.columns = range(consensus_df.shape[1])\n",
    "        consensus_df = consensus_df.astype({0:str,1:int,2:int})\n",
    "        consensus_df = consensus_df[[0,1,2]]\n",
    "        ann = consensus_df[[0,1,2,0,1,2]]\n",
    "        mean_sc = max_sc/len(consensus_df)\n",
    "        meta.append([sp,chrom,consensus_df,tad_len,mean_sc,g2])\n",
    "        print(sp,chrom,g2,tad_len,mean_sc)\n",
    "        if tad_len < sizes[-1]:\n",
    "            print('bye')\n",
    "            break\n",
    "    print('end of chrom',sp,chrom)\n",
    "    #     print (\"{:g} s\".format(time.process_time() - start_time))\n",
    "meta = np.array(meta)\n",
    "\n",
    "for sp, s in product(species,sizes):\n",
    "    meta1 = meta[(meta[:,0]==sp)]\n",
    "\n",
    "    meta4 = []\n",
    "    for chrom in chroms:\n",
    "        meta2 = meta1[(meta1[:,1]==chrom)]\n",
    "        meta3 = meta2[np.argmin(abs(meta2[:,3]-s))]\n",
    "        meta3 = meta3.tolist()\n",
    "        meta4.append(meta3)\n",
    "    meta4 = np.array(meta4)\n",
    "    meta5 = meta4[:,2]\n",
    "    bed = np.concatenate(meta5,axis=0)\n",
    "    ann = np.concatenate([bed,bed],axis=1)\n",
    "    \n",
    "    path = '/home/al/Vp/0402/'\n",
    "    try:\n",
    "        makedirs(path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    stat = meta4[:,[0,1,3,4,5]]\n",
    "    stat1 = np.concatenate(stat,axis=0)\n",
    "#     np.savetxt('/home/al/Vp/0402/'+sp+'_'+str(s)+'stat.bed',stat1, delimiter='\\t', newline='\\n',fmt='%s')\n",
    "#     np.savetxt('/home/al/Vp/0402/'+sp+'_'+str(s)+'.bed',bed, delimiter='\\t', newline='\\n',fmt='%s')\n",
    "#     np.savetxt('/home/al/Vp/0402/'+sp+'_'+str(s)+'.ann',ann, delimiter='\\t', newline='\\n',fmt='%s')\n",
    "print (\"{:g} s\".format(time.process_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
